{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: neo4j in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (5.22.0)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.41.2)\n",
      "Requirement already satisfied: pytz in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from neo4j) (2024.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install neo4j transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'The model meta-llama/Meta-Llama-3-8B is too large to be loaded automatically (16GB > 10GB). Please use Spaces (https://huggingface.co/spaces) or Inference Endpoints (https://huggingface.co/inference-endpoints).'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from neo4j import GraphDatabase\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "# Set Hugging Face access token\n",
    "hf_token = \"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\"  # Replace with your actual access token\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"newpassword\"))\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "\n",
    "# Initialize the LLM (using Hugging Face Transformers for example)\n",
    "# llm = pipeline(\"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\")\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "\n",
    "def query_knowledge_graph(tx, person_name):\n",
    "    # Query to get person details and related information\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Person {name: $person_name})-[:HAS_CONDITION]->(c:Condition)\n",
    "    MATCH (p)-[:LAST_KNOWN_LOCATION]->(l:Location)\n",
    "    MATCH (p)-[:HAS_FREE_RESPONSE]->(f:FreeResponse)\n",
    "    RETURN p, c, l, f\n",
    "    \"\"\"\n",
    "    result = tx.run(query, person_name=person_name)\n",
    "    return result.single()\n",
    "\n",
    "def anonymize_data(person_data):\n",
    "    # Replace PII with anonymized data\n",
    "    person_data[\"p\"][\"name\"] = \"Anonymized Person\"\n",
    "    return person_data\n",
    "\n",
    "def generate_recommendations(person_data):\n",
    "    # Anonymize data\n",
    "    # anonymized_data = anonymize_data(person_data)\n",
    "\n",
    "    # Extract data from the query result\n",
    "    # person = anonymized_data[\"p\"]\n",
    "    # condition = anonymized_data[\"c\"]\n",
    "    # location = anonymized_data[\"l\"]\n",
    "    # free_response = anonymized_data[\"f\"]\n",
    "    person = person_data[\"p\"]\n",
    "    condition = person_data[\"c\"]\n",
    "    location = person_data[\"l\"]\n",
    "    free_response = person_data[\"f\"]\n",
    "    \n",
    "    # Prepare the input for the LLM\n",
    "    input_text = f\"\"\"\n",
    "    Person: {person['name']}, Age: {person['age']}, Health Condition: {condition['name']}\n",
    "    Last Known Location: {location['name']}\n",
    "    Additional Information: {free_response['text']}\n",
    "    \n",
    "    Based on the above information, where should the search and rescue team look for the person?\n",
    "    \"\"\"\n",
    "    output = query({\n",
    "\t\"inputs\": \"Can you please let us know more details about your \",})\n",
    "    return output\n",
    "    \n",
    "    # Generate recommendations using the LLM\n",
    "    # recommendations = llm(input_text, max_length=200, truncation= True, num_return_sequences=1)\n",
    "    # return recommendations[0][\"generated_text\"]\n",
    "\n",
    "# Connect to Neo4j and get person data\n",
    "with driver.session() as session:\n",
    "    person_name = \"John Doe\"  # Example person name\n",
    "    person_data = session.execute_read(query_knowledge_graph, person_name)\n",
    "\n",
    "# Generate recommendations based on the person data\n",
    "try:\n",
    "    recommendations = generate_recommendations(person_data)\n",
    "    print(recommendations)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to generate recommendations: {e}\")\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.3.1)\n",
      "Requirement already satisfied: transformers in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.41.2)\n",
      "Requirement already satisfied: neo4j in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (5.22.0)\n",
      "Requirement already satisfied: sentencepiece in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: pytz in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from neo4j) (2024.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch transformers neo4j sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to generate recommendations: 'Node' object does not support item assignment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3d/gtb88cpj5zv4nk0q2yhbb0700000gn/T/ipykernel_89805/1683465013.py:57: DeprecationWarning: read_transaction has been renamed to execute_read\n",
      "  person_data = session.read_transaction(query_knowledge_graph, person_name)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Placeholder for the Ollama call function\n",
    "def call_ollama_model(input_text):\n",
    "    # Replace this with the actual function to call the Ollama model\n",
    "    # For example, if Ollama provides an API or a function, use it here\n",
    "    response = requests.post(\"http://localhost:8000/generate\", json={\"text\": input_text})\n",
    "    return response.json()[\"generated_text\"]\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"newpassword\"))\n",
    "\n",
    "def query_knowledge_graph(tx, person_name):\n",
    "    # Query to get person details and related information\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Person {name: $person_name})-[:HAS_CONDITION]->(c:Condition)\n",
    "    MATCH (p)-[:LAST_KNOWN_LOCATION]->(l:Location)\n",
    "    MATCH (p)-[:HAS_FREE_RESPONSE]->(f:FreeResponse)\n",
    "    RETURN p, c, l, f\n",
    "    \"\"\"\n",
    "    result = tx.run(query, person_name=person_name)\n",
    "    return result.single()\n",
    "\n",
    "def anonymize_data(person_data):\n",
    "    # Replace PII with anonymized data\n",
    "    person_data[\"p\"][\"name\"] = \"Anonymized Person\"\n",
    "    return person_data\n",
    "\n",
    "def generate_recommendations(person_data):\n",
    "    # Anonymize data\n",
    "    anonymized_data = anonymize_data(person_data)\n",
    "\n",
    "    # Extract data from the query result\n",
    "    person = anonymized_data[\"p\"]\n",
    "    condition = anonymized_data[\"c\"]\n",
    "    location = anonymized_data[\"l\"]\n",
    "    free_response = anonymized_data[\"f\"]\n",
    "    \n",
    "    # Prepare the input for the LLM\n",
    "    input_text = f\"\"\"\n",
    "    Person: {person['name']}, Age: {person['age']}, Health Condition: {condition['name']}\n",
    "    Last Known Location: {location['name']}\n",
    "    Additional Information: {free_response['text']}\n",
    "    \n",
    "    Based on the above information, where should the search and rescue team look for the person?\n",
    "    \"\"\"\n",
    "    \n",
    "    # Call the Ollama model to generate recommendations\n",
    "    recommendations = call_ollama_model(input_text)\n",
    "    return recommendations\n",
    "\n",
    "# Connect to Neo4j and get person data\n",
    "with driver.session() as session:\n",
    "    person_name = \"John Doe\"  # Example person name\n",
    "    person_data = session.read_transaction(query_knowledge_graph, person_name)\n",
    "\n",
    "# Generate recommendations based on the person data\n",
    "try:\n",
    "    recommendations = generate_recommendations(person_data)\n",
    "    print(recommendations)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to generate recommendations: {e}\")\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from groq) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from groq) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from groq) (2.7.4)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from groq) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.7)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.18.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, multi-agent systems (MAS) can be highly useful for search and rescue (SAR) missions. MAS involve multiple autonomous agents, such as robots, drones, or vehicles, that work together to achieve a common goal. In the context of SAR, MAS can significantly enhance the efficiency and effectiveness of search and rescue operations. Here are some ways MAS can be useful for SAR missions:\n",
      "\n",
      "1. **Scalability**: MAS can cover a larger search area more quickly and efficiently than a single agent, making them ideal for searching large territories or disaster scenarios.\n",
      "2. **Diverse sensor capabilities**: Each agent can be equipped with different sensors (e.g., visual, thermal, acoustic) to gather diverse data, increasing the chances of detecting survivors or hazards.\n",
      "3. **Improved situational awareness**: By sharing information and coordinating their actions, agents can build a more comprehensive understanding of the search environment, reducing the risk of missing critical information.\n",
      "4. **Autonomy and adaptability**: MAS can adapt to changing environmental conditions, such as weather or terrain, and adjust their search strategy accordingly.\n",
      "5. **Enhanced decision-making**: Agents can collaborate to make more informed decisions about search priorities, resource allocation, and task assignment.\n",
      "6. **Reduced human risk**: By deploying agents to search hazardous areas, human responders can be kept safe and focused on more critical tasks, such as medical assistance or debris removal.\n",
      "7. **Improved communication**: MAS can establish communication networks to facilitate information exchange between agents, ground control, and responding teams, ensuring that critical information is shared promptly.\n",
      "8. **Increased precision**: Agents can work together to pinpoint the location of survivors or hazards, reducing the risk of false positives or negatives.\n",
      "9. **Flexibility and reconfigurability**: MAS can be easily reconfigured to accommodate changing mission requirements or unexpected events, such as the discovery of new hazards or the need to prioritize specific areas.\n",
      "10. **Cost-effectiveness**: Deploying multiple agents can be more cost-effective than using a single, highly capable agent or relying on human search teams alone.\n",
      "\n",
      "Example applications of MAS in SAR missions include:\n",
      "\n",
      "1. **Disaster response**: Deploying agents to search for survivors in rubble or debris after earthquakes, hurricanes, or other disasters.\n",
      "2. **Wilderness search**: Using agents to search for missing persons in forests, mountains, or other wilderness areas.\n",
      "3. **Maritime search and rescue**: Deploying agents to search for survivors at sea or in coastal areas.\n",
      "4. **Industrial accidents**: Using agents to search for survivors in industrial settings, such as collapsed buildings or chemical plants.\n",
      "\n",
      "While MAS offers many benefits for SAR missions, there are also challenges to consider, such as:\n",
      "\n",
      "1. **Complexity**: Coordinating multiple agents requires sophisticated algorithms and communication systems.\n",
      "2. **Interoperability**: Ensuring that agents from different manufacturers or organizations can work together seamlessly.\n",
      "3. **Safety and security**: Ensuring that agents do not pose a risk to humans or other agents.\n",
      "4. **Data integration and analysis**: Combining and analyzing data from multiple agents to generate actionable insights.\n",
      "\n",
      "By addressing these challenges, MAS can become an essential tool for enhancing search and rescue operations, ultimately leading to faster response times, improved situational awareness, and increased safety for both responders and survivors."
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq(api_key= \"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\" )\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"llama3-70b-8192\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Are multi agent systems useful for search and rescue missions?\"\n",
    "        }\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=1024,\n",
    "    top_p=1,\n",
    "    stream=True,\n",
    "    stop=None,\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content or \"\", end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Knowledge Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data: belal elshenety is a dementia patient who is 72 years old he was last seen on dexter lawn at cal poly slo\n",
      "Extracted entities and relationships: MERGE (p:Person {name: \"belal elshenety\", age: 72})\n",
      "MERGE (c:Condition {name: \"dementia\"})\n",
      "MERGE (l:Location {name: \"dexter lawn\"})\n",
      "MERGE (loc:Location {name: \"cal poly slo\"})\n",
      "MERGE (fr:FreeResponse {text: \"belal elshenety is a dementia patient who is 72 years old he was last seen on dexter lawn at cal poly slo\"})\n",
      "MERGE (p)-[:HAS_CONDITION]->(c)\n",
      "MERGE (p)-[:LAST_KNOWN_LOCATION]->(l)\n",
      "MERGE (l)-[:PART_OF]->(loc)\n",
      "MERGE (p)-[:HAS_FREE_RESPONSE]->(fr)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3d/gtb88cpj5zv4nk0q2yhbb0700000gn/T/ipykernel_89805/2875412168.py:76: DeprecationWarning: write_transaction has been renamed to execute_write\n",
      "  session.write_transaction(create_knowledge_graph, entities_relationships)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge graph updated successfully.\n",
      "Processing data: mohsen is 22 years old he has diabetes and was last seen at the library\n",
      "Extracted entities and relationships: MERGE (p:Person {name: \"mohsen\", age: 22})\n",
      "MERGE (c:Condition {name: \"diabetes\"})\n",
      "MERGE (l:Location {name: \"library\"})\n",
      "MERGE (fr:FreeResponse {text: \"mohsen is 22 years old he has diabetes and was last seen at the library\"})\n",
      "MERGE (p)-[:HAS_CONDITION]->(c)\n",
      "MERGE (p)-[:LAST_KNOWN_LOCATION]->(l)\n",
      "MERGE (p)-[:HAS_FREE_RESPONSE]->(fr)\n",
      "Knowledge graph updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from neo4j import GraphDatabase\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "groq_client = Groq(api_key=\"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\")\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"newpassword\"))\n",
    "\n",
    "def preprocess_data(text):\n",
    "    \"\"\"Preprocess the input text by converting it to lowercase, removing extra whitespace, and stripping out non-alphanumeric characters.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def extract_entities_and_relationships(text):\n",
    "    \"\"\"Use the LLM to extract entities and relationships from the preprocessed text and format them as a Cypher query.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Extract entities and relationships from the following text and format them as a Cypher query to update a Neo4j knowledge graph. Do not include any explanations or notes, only the Cypher query:\n",
    "    Text: \"{text}\"\n",
    "    \n",
    "    The Cypher query should create nodes and relationships based on the extracted entities. Ensure to use the following node labels and relationship types:\n",
    "    - Person (name, age)\n",
    "    - Condition (name)\n",
    "    - Location (name)\n",
    "    - FreeResponse (text)\n",
    "    - Relationships: HAS_CONDITION, LAST_KNOWN_LOCATION, HAS_FREE_RESPONSE\n",
    "\n",
    "    Make sure all arrows are coming out of the person to other nodes.\n",
    "    Do not include a single word that is not part of cypher syntax, even the words \"here is the cypher query\" should not be included.\n",
    "    Cypher query:\n",
    "    \"\"\"\n",
    "    \n",
    "    completion = groq_client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.7,\n",
    "        max_tokens=512,\n",
    "        top_p=0.9,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    response = \"\"\n",
    "    for chunk in completion:\n",
    "        response += chunk.choices[0].delta.content or \"\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "def create_knowledge_graph(tx, entities_relationships):\n",
    "    \"\"\"Execute the generated Cypher query to update the Neo4j knowledge graph.\"\"\"\n",
    "    query = f\"\"\"\n",
    "    // Generated Cypher query from LLM response\n",
    "    {entities_relationships}\n",
    "    \"\"\"\n",
    "    tx.run(query)\n",
    "\n",
    "# Example data\n",
    "free_response_data = [\n",
    "    \"Belal Elshenety is a dementia patient who is 72 years old. He was last seen on Dexter Lawn at Cal Poly SLO\",\n",
    "    \"Mohsen is 22 years old. He has diabetes and was last seen at the library.\",\n",
    "]\n",
    "\n",
    "# Preprocess the free-text responses\n",
    "preprocessed_data = [preprocess_data(text) for text in free_response_data]\n",
    "\n",
    "# Connect to Neo4j and build the knowledge graph\n",
    "with driver.session() as session:\n",
    "    for data in preprocessed_data:\n",
    "        print(f\"Processing data: {data}\")\n",
    "        entities_relationships = extract_entities_and_relationships(data)\n",
    "        print(f\"Extracted entities and relationships: {entities_relationships}\")\n",
    "        try:\n",
    "            session.write_transaction(create_knowledge_graph, entities_relationships)\n",
    "            print(\"Knowledge graph updated successfully.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to create knowledge graph: {e}\")\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate LLM response based on knowledge graph query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/neo4j/_sync/work/result.py:604: UserWarning: Expected a result with a single record, but found multiple.\n",
      "  warn(\"Expected a result with a single record, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided, I recommend that the search and rescue team focus their search on areas near water and rivers adjacent to Dexter Lawn at Cal Poly SLO.\n",
      "\n",
      "My recommendation is based on the \"Condition-Specific Guidelines\" that suggest searching in places near water and rivers, which is a common guideline for searching for individuals with dementia. This is because people with dementia may be attracted to water sources, such as rivers, lakes, or ponds, and may wander towards them.\n",
      "\n",
      "Additionally, since the last known location of Belal Elshenety is Dexter Lawn at Cal Poly SLO, the search team should start by thoroughly searching the surrounding areas, including any nearby water sources, such as creeks, rivers, or ponds.\n",
      "\n",
      "Specifically, the search team should consider searching:\n",
      "\n",
      "* Any nearby rivers, creeks, or streams that may be adjacent to Dexter Lawn or Cal Poly SLO campus.\n",
      "* Ponds, lakes, or other bodies of water within a reasonable distance from Dexter Lawn.\n",
      "* Areas with easy access to water sources, such as walking trails or parks near rivers or creeks.\n",
      "\n",
      "By prioritizing these areas, the search team can increase the chances of locating Belal Elshenety safely."
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from neo4j import GraphDatabase\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "groq_client = Groq(api_key=\"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\")\n",
    "\n",
    "# Initialize Neo4j driver\n",
    "uri = \"bolt://localhost:7687\"\n",
    "driver = GraphDatabase.driver(uri, auth=(\"neo4j\", \"newpassword\"))\n",
    "\n",
    "def query_knowledge_graph(tx, person_name):\n",
    "    query = \"\"\"\n",
    "    MATCH (p:Person {name: $person_name})-[:HAS_CONDITION]->(c:Condition)\n",
    "    MATCH (p)-[:LAST_KNOWN_LOCATION]->(l:Location)\n",
    "    MATCH (p)-[:HAS_FREE_RESPONSE]->(f:FreeResponse)\n",
    "    RETURN p, c, l, f\n",
    "    \"\"\"\n",
    "    result = tx.run(query, person_name=person_name)\n",
    "    return result.single()\n",
    "\n",
    "def anonymize_data(person_data):\n",
    "    anonymized_data = {\n",
    "        \"p\": {\n",
    "            \"name\": \"Anonymized Person\",\n",
    "            \"age\": person_data[\"p\"][\"age\"]\n",
    "        },\n",
    "        \"c\": {\n",
    "            \"name\": person_data[\"c\"][\"name\"],\n",
    "            \"recommendation\": person_data[\"c\"][\"recommendation\"] if \"recommendation\" in person_data[\"c\"] else \"No specific guidelines available.\"\n",
    "        },\n",
    "        \"l\": {\n",
    "            \"name\": person_data[\"l\"][\"name\"]\n",
    "        },\n",
    "        \"f\": {\n",
    "            \"text\": person_data[\"f\"][\"text\"]\n",
    "        }\n",
    "    }\n",
    "    return anonymized_data\n",
    "\n",
    "def generate_recommendations(person_data):\n",
    "    anonymized_data = anonymize_data(person_data)\n",
    "\n",
    "    person = anonymized_data[\"p\"]\n",
    "    condition = anonymized_data[\"c\"]\n",
    "    location = anonymized_data[\"l\"]\n",
    "    free_response = anonymized_data[\"f\"]\n",
    "    condition_guidelines = condition[\"recommendation\"]\n",
    "\n",
    "    input_text = f\"\"\"\n",
    "    Person: {person['name']}, Age: {person['age']}, Health Condition: {condition['name']}\n",
    "    Last Known Location: {location['name']}\n",
    "    Additional Information: {free_response['text']}\n",
    "    Condition-Specific Guidelines: {condition_guidelines}\n",
    "    \n",
    "    Based on the above information, where should the search and rescue team look for the person?\n",
    "\n",
    "    Make sure to specify what you based your recommendations on.\n",
    "    \"\"\"\n",
    "    \n",
    "    completion = groq_client.chat.completions.create(\n",
    "        model=\"llama3-70b-8192\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": input_text\n",
    "            }\n",
    "        ],\n",
    "        temperature=0.8,\n",
    "        max_tokens=1024,\n",
    "        top_p=1,\n",
    "        stream=True,\n",
    "        stop=None,\n",
    "    )\n",
    "\n",
    "    for chunk in completion:\n",
    "        print(chunk.choices[0].delta.content or \"\", end=\"\")\n",
    "\n",
    "# Connect to Neo4j and get person data\n",
    "with driver.session() as session:\n",
    "    person_name = \"belal elshenety\"  # Example person name\n",
    "    person_data = session.execute_read(query_knowledge_graph, person_name)\n",
    "\n",
    "# Generate recommendations based on the person data\n",
    "try:\n",
    "    generate_recommendations(person_data)\n",
    "except Exception as e:\n",
    "    print(f\"Failed to generate recommendations: {e}\")\n",
    "\n",
    "driver.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
